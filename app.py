# app.py ã¨ã„ã†åå‰ã§ä¿å­˜ã—ã¦ãã ã•ã„

import streamlit as st
import pandas as pd
from googlesearch import search
import requests
from bs4 import BeautifulSoup
import time
import random
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- â–¼â–¼â–¼ åŸºæœ¬è¨­å®š â–¼â–¼â–¼ ---
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',
]
EXCLUDED_DOMAINS = ['ipros', 'hotfrog', 'baseconnect', 'musubu', 'appletech', 'kensetumap', 'ja.wikipedia.org']
EXCLUDED_URL_PATHS = ['/contact', '/inquiry', '/form', '/privacy', '/policy']

# --- â˜…â˜…â˜… æŠ½å‡ºå°‚é–€ã®é–¢æ•°ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¨ãƒ³ã‚¸ãƒ³ï¼‰ â˜…â˜…â˜… ---
def extract_data_from_soup(soup):
    data = {'ä¼šç¤¾å': '', 'ä»£è¡¨è€…å': '', 'ä½æ‰€': '', 'è³‡æœ¬é‡‘': '', 'å¾“æ¥­å“¡æ•°': ''}
    
    def get_full_text(tag):
        if not tag: return ""
        return ' '.join(tag.find_all(string=True, recursive=True)).strip()

    # ã‚¨ãƒ³ã‚¸ãƒ³1ï¼šæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã€ãƒªã‚¹ãƒˆï¼‰ã‹ã‚‰æœ€å„ªå…ˆã§æŠ½å‡º
    for label_tag in soup.find_all(['th', 'dt']):
        key_text = get_full_text(label_tag)
        value_tag = label_tag.find_next_sibling(['td', 'dd'])
        if value_tag:
            value_text = get_full_text(value_tag)
            if not data.get('ä¼šç¤¾å') and any(k in key_text for k in ['ä¼šç¤¾å', 'å•†å·']): data['ä¼šç¤¾å'] = value_text
            if not data.get('ä»£è¡¨è€…å') and any(k in key_text for k in ['ä»£è¡¨è€…', 'ä»£è¡¨å–ç· å½¹']): data['ä»£è¡¨è€…å'] = value_text
            if not data.get('ä½æ‰€') and any(k in key_text for k in ['æ‰€åœ¨åœ°', 'æœ¬ç¤¾']): data['ä½æ‰€'] = value_text
            if not data.get('è³‡æœ¬é‡‘') and 'è³‡æœ¬é‡‘' in key_text: data['è³‡æœ¬é‡‘'] = value_text
            if not data.get('å¾“æ¥­å“¡æ•°') and 'å¾“æ¥­å“¡' in key_text: data['å¾“æ¥­å“¡æ•°'] = value_text

    # ã‚¨ãƒ³ã‚¸ãƒ³2ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¿‘å‚æ¤œç´¢ã§ã€æœªå–å¾—ã®é …ç›®ã‚’è£œå®Œ
    def get_value_by_keyword_proximity(target_soup, keywords):
        for keyword in keywords:
            found_element = target_soup.find(string=re.compile(re.escape(keyword), re.IGNORECASE))
            if not found_element: continue
            for i in range(3):
                container = found_element.find_parent() if i == 0 else container.find_parent()
                if not container: break
                container_text = ' '.join(container.get_text(strip=True).split())
                value_candidate = re.sub(re.escape(keyword), '', container_text, flags=re.IGNORECASE).strip()
                if value_candidate and 1 < len(value_candidate) < 100: return value_candidate
        return ""

    if any(not data.get(key) for key in ['ä¼šç¤¾å', 'ä»£è¡¨è€…å', 'ä½æ‰€']):
        if not data['ä¼šç¤¾å']: data['ä¼šç¤¾å'] = get_value_by_keyword_proximity(soup, ['ä¼šç¤¾å', 'å•†å·', 'ç¤¾å'])
        if not data['ä»£è¡¨è€…å']: data['ä»£è¡¨è€…å'] = get_value_by_keyword_proximity(soup, ['ä»£è¡¨å–ç· å½¹ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'ä»£è¡¨è€…'])
        if not data['ä½æ‰€']: data['ä½æ‰€'] = get_value_by_keyword_proximity(soup, ['æ‰€åœ¨åœ°', 'æœ¬ç¤¾æ‰€åœ¨åœ°', 'ä½æ‰€'])
        if not data['è³‡æœ¬é‡‘']: data['è³‡æœ¬é‡‘'] = get_value_by_keyword_proximity(soup, ['è³‡æœ¬é‡‘'])
        if not data['å¾“æ¥­å“¡æ•°']: data['å¾“æ¥­å“¡æ•°'] = get_value_by_keyword_proximity(soup, ['å¾“æ¥­å“¡æ•°', 'å¾“æ¥­å“¡'])

    # æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    titles_rep = ['ä»£è¡¨å–ç· å½¹ç¤¾é•·', 'ä»£è¡¨å–ç· å½¹', 'ä»£è¡¨ç¤¾å“¡', 'ä»£è¡¨', 'ç¤¾é•·', 'ï¼š', ':']
    titles_other = ['å–ç· å½¹', 'ç›£æŸ»å½¹', 'åŸ·è¡Œå½¹å“¡']
    if data.get('ä»£è¡¨è€…å'):
        for title in titles_other:
            if title in data['ä»£è¡¨è€…å']: data['ä»£è¡¨è€…å'] = data['ä»£è¡¨è€…å'].split(title)[0]
        for title in titles_rep:
            data['ä»£è¡¨è€…å'] = data['ä»£è¡¨è€…å'].replace(title, '')
        data['ä»£è¡¨è€…å'] = data['ä»£è¡¨è€…å'].strip()
    for key, value in data.items():
        if isinstance(value, str):
            cleaned_value = re.sub(r'TEL.*|FAX.*|URL.*|E-mail.*|â†’.*|åœ°å›³.*|ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰.*|ã€’\d{3}-\d{4}', '', value, flags=re.IGNORECASE)
            data[key] = ' '.join(cleaned_value.split())
    return data

def validate_phone_in_html(html_content, phone_number):
    soup = BeautifulSoup(html_content, 'html.parser')
    body_tag = soup.find('body')
    if not body_tag: return False, None
    body_text = body_tag.get_text()
    translation_table = str.maketrans('ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼ˆï¼‰ï¼â€ã€€', '0123456789()-- ')
    normalized_body_text = body_text.translate(translation_table)
    normalized_body_text = ''.join(filter(str.isdigit, normalized_body_text))
    if phone_number in normalized_body_text: return True, soup
    return False, None

def find_valid_url(query, phone_number, status_container):
    status_container.info(f"æ¤œç´¢ä¸­: {query}")
    try:
        search_results = search(query, num_results=5, lang='ja', sleep_interval=3)
        for temp_url in search_results:
            if any(path in temp_url for path in EXCLUDED_URL_PATHS) or any(domain in temp_url for domain in EXCLUDED_DOMAINS):
                status_container.warning(f"é™¤å¤–å¯¾è±¡ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {temp_url}")
                continue
            status_container.info(f"URLå€™è£œç™ºè¦‹: {temp_url}")
            headers = {'User-Agent': random.choice(USER_AGENTS)}
            response = requests.get(temp_url, headers=headers, timeout=15)
            response.raise_for_status()
            is_validated, validated_soup = validate_phone_in_html(response.text, phone_number)
            if is_validated:
                status_container.success("æ¤œè¨¼æˆåŠŸï¼ã“ã®URLã‚’æ¡ç”¨ã—ã¾ã™ã€‚")
                return temp_url, validated_soup
            else:
                status_container.warning("æ¤œè¨¼å¤±æ•—ï¼šé›»è©±ç•ªå·ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚")
                time.sleep(random.uniform(1, 2.5))
    except Exception as e:
        if "429" in str(e): raise e
        else: st.error(f"æ¤œç´¢ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
    return None, None

def run_scraping_process(uploaded_file, status_container):
    df = pd.read_csv(uploaded_file, dtype=str, encoding="utf-8").fillna('')
    phone_column_name = 'é›»è©±ç•ªå·' if 'é›»è©±ç•ªå·' in df.columns else 'ç™ºä¿¡å…ˆé›»è©±ç•ªå·'
    if phone_column_name not in df.columns:
        st.error(f"ã‚¨ãƒ©ãƒ¼: CSVã« '{phone_column_name}' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    results = []
    driver = None
    total_rows = len(df)
    google_blocked = False
    
    try:
        for index, row in df.iterrows():
            progress = (index + 1) / total_rows
            yield progress, f"[{index + 1}/{total_rows}] å‡¦ç†ä¸­: {row.get(phone_column_name, '')}", None

            phone_number = row.get(phone_column_name, '')
            if not phone_number:
                results.append({'URL': 'é›»è©±ç•ªå·ãªã—', 'ä¼šç¤¾å': '', 'ä»£è¡¨è€…å': '', 'ä½æ‰€': '', 'è³‡æœ¬é‡‘': '', 'å¾“æ¥­å“¡æ•°': ''})
                continue

            target_url, final_soup = None, None
            phone_formats = []
            if len(phone_number) >= 10:
                if len(phone_number) == 11 and phone_number.startswith(('070', '080', '090')): phone_formats.append(f'"{phone_number[:3]}-{phone_number[3:7]}-{phone_number[7:]}"')
                elif len(phone_number) == 10:
                    phone_formats.append(f'"{phone_number[:3]}-{phone_number[3:6]}-{phone_number[6:]}"')
                    phone_formats.append(f'"{phone_number[:4]}-{phone_number[4:6]}-{phone_number[6:]}"')
            phone_formats.append(f'"{phone_number}"')
            phone_search_group = f"({' OR '.join(phone_formats)})"
            
            query1_intitle = f'{phone_search_group} (intitle:"ä¼šç¤¾æ¦‚è¦" OR intitle:"ä¼šç¤¾æ¡ˆå†…" OR intitle:"ä¼æ¥­æƒ…å ±" OR intitle:"ä¼šç¤¾æƒ…å ±")'
            query2_inurl = f'{phone_search_group} (inurl:company OR inurl:profile OR inurl:about OR inurl:corporate)'
            query3_broad = phone_search_group
            
            try:
                status_container.info("[ç¬¬1æ®µéš] ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œ...")
                target_url, final_soup = find_valid_url(query1_intitle, phone_number, status_container)
                
                if not target_url:
                    wait_time = random.uniform(2, 4)
                    status_container.warning(f"ç¬¬1æ®µéšã§è¦‹ã¤ã‹ã‚‰ãšã€{wait_time:.2f}ç§’å¾…æ©Ÿã—ã¦URLæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    time.sleep(wait_time)
                    target_url, final_soup = find_valid_url(query2_inurl, phone_number, status_container)

                if not target_url:
                    wait_time = random.uniform(2, 4)
                    status_container.warning(f"ç¬¬2æ®µéšã§è¦‹ã¤ã‹ã‚‰ãšã€{wait_time:.2f}ç§’å¾…æ©Ÿã—ã¦åºƒåŸŸæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    time.sleep(wait_time)
                    target_url, final_soup = find_valid_url(query3_broad, phone_number, status_container)
            except Exception as e:
                if "429" in str(e):
                    st.error("Googleã‹ã‚‰ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
                    google_blocked = True
                else: st.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
            
            if google_blocked:
                results.append({'URL': 'ãƒ–ãƒ­ãƒƒã‚¯ã«ã‚ˆã‚Šä¸­æ–­', 'ä¼šç¤¾å': '', 'ä»£è¡¨è€…å': '', 'ä½æ‰€': '', 'è³‡æœ¬é‡‘': '', 'å¾“æ¥­å“¡æ•°': ''})
                break

            extracted_info = {}
            if target_url and final_soup:
                extracted_info = extract_data_from_soup(final_soup)
                
                if not all(extracted_info.get(k) for k in ['ä¼šç¤¾å', 'ä»£è¡¨è€…å', 'ä½æ‰€']):
                    status_container.warning("æŠ½å‡ºä¸ååˆ†ã€‚JavaScriptå¯¾å¿œã®ãŸã‚Seleniumã§å†è©¦è¡Œã—ã¾ã™ã€‚")
                    if driver is None:
                        try:
                            options = Options(); options.add_argument('--headless'); options.add_argument('--disable-gpu')
                            options.add_argument("user-agent=" + random.choice(USER_AGENTS))
                            service = Service(ChromeDriverManager().install())
                            driver = webdriver.Chrome(service=service, options=options)
                        except Exception as e: st.error(f"WebDriverã®èµ·å‹•ã«å¤±æ•—: {e}")
                    if driver:
                        try:
                            driver.get(target_url); time.sleep(5)
                            selenium_soup = BeautifulSoup(driver.page_source, 'html.parser')
                            extracted_info = extract_data_from_soup(selenium_soup)
                        except Exception as e: st.error(f"Seleniumå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            
            result_data = {'URL': target_url or 'è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“', **extracted_info}
            results.append(result_data)
            time.sleep(random.uniform(5, 10))
    finally:
        if driver: driver.quit()

    result_df = pd.DataFrame(results)
    df_processed = df.head(len(results))
    df_original = df_processed.drop(columns=[col for col in ['URL', 'ä¼šç¤¾å', 'ä»£è¡¨è€…å', 'ä½æ‰€', 'è³‡æœ¬é‡‘', 'å¾“æ¥­å“¡æ•°'] if col in df.columns])
    output_df = pd.concat([df_original.reset_index(drop=True), result_df.reset_index(drop=True)], axis=1)
    yield 1.0, "å®Œäº†ï¼", output_df

# --- â–¼â–¼â–¼ Streamlit UIéƒ¨åˆ† â–¼â–¼â–¼ ---
st.set_page_config(page_title="ä¼æ¥­æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¢ãƒ—ãƒª", layout="wide")
st.title('ğŸ¤– ä¼æ¥­æƒ…å ± è‡ªå‹•å–å¾—ã‚¢ãƒ—ãƒª')
st.markdown("CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã‚‹é›»è©±ç•ªå·ã‚’å…ƒã«ã€ä¼æ¥­ã®å…¬å¼ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’æ¤œç´¢ã—ã€ä¼šç¤¾æƒ…å ±ã‚’è‡ªå‹•ã§å–å¾—ã—ã¾ã™ã€‚")

results_placeholder = st.empty()
download_placeholder = st.empty()

uploaded_file = st.file_uploader("é›»è©±ç•ªå·ã‚’å«ã‚€CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type="csv")

if uploaded_file is not None:
    if st.button('å‡¦ç†é–‹å§‹'):
        results_placeholder.empty()
        download_placeholder.empty()
        
        progress_bar = st.progress(0)
        status_container = st.expander("è©³ç´°ãƒ­ã‚°", expanded=True)
        status_container.info("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã€‚å®Œäº†ã¾ã§ãŠå¾…ã¡ãã ã•ã„...")
        final_df = None

        for progress, message, df_result in run_scraping_process(uploaded_file, status_container):
            progress_bar.progress(progress)
            status_container.info(message)
            if df_result is not None:
                final_df = df_result

        st.success("ğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
        results_placeholder.dataframe(final_df)
        csv_data = final_df.to_csv(index=False, encoding='utf_8_sig').encode('utf_8_sig')
        download_placeholder.download_button(
            label="çµæœã‚’CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name='æœ€çµ‚çµæœ_ãƒªã‚¹ãƒˆ.csv',
            mime='text/csv',
        )